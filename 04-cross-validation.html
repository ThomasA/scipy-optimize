<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="pandoc">
    <title>Model evaulation with cross-validation</title>
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" type="text/css" href="css/bootstrap/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="css/bootstrap/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="css/swc.css" />
    <meta charset="UTF-8" />
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body class="lesson">
    <div class="container card">
      <div class="banner">
      
      </div>
      <article>
      <div class="row">
        <div class="col-md-10 col-md-offset-1">
                    <h1 class="title">Model evaulation with cross-validation</h1>
          <h1 id="model-evaluation-with-cross-validation">Model evaluation with cross-validation</h1>
<section class="objectives panel panel-warning">
<div class="panel-heading">
<h2 id="learning-objectives"><span class="glyphicon glyphicon-certificate"></span>Learning Objectives</h2>
</div>
<div class="panel-body">
<ul>
<li>Learners can define and identify overfitting.</li>
<li>Learners can use split-half cross-validation to evaluate model error.</li>
</ul>
</div>
</section>
<p>How do we know that a model is <em>good enough</em>?</p>
<p>In the previous section, we managed to reduce the SSE substantially relative to the linear model, by fitting a non-linear, but it’s still not perfect. How about trying a more complicated model, with more parameters? Another function that is often used to fit psychometric data is the Weibull cumulative distribution function, named after the great Swedish mathematician and engineer <a href="http://en.wikipedia.org/wiki/Waloddi_Weibull">Waloddi Weibull</a></p>
<pre class="sourceCode python"><code class="sourceCode python">
<span class="kw">def</span> weibull(x, threshx, slope, lower_asymp, upper_asymp):
    <span class="co">&quot;&quot;&quot;</span>
<span class="co">    The Weibull cumulative distribution function</span>

<span class="co">    Parameters</span>
<span class="co">    ----------</span>
<span class="co">    x : float or array</span>
<span class="co">       The values of x over which to evaluate the cumulative Weibull function</span>

<span class="co">    threshx : float</span>
<span class="co">       The value of x at the deflection point of the function.</span>
<span class="co">       For a lower_asymp set to 0.5, this is at approximately y=0.81  </span>

<span class="co">    slope : float</span>
<span class="co">        The slope of the function at the deflection point.</span>

<span class="co">    lower_asymp : float</span>
<span class="co">        The lower asymptote of the function</span>

<span class="co">    upper_asymp : float</span>
<span class="co">        The upper asymptote of the function.</span>
<span class="co">    &quot;&quot;&quot;</span>
    threshy = <span class="dv">1</span> - (lower_asymp * np.exp(-<span class="dv">1</span>))
    k = (-np.log((<span class="dv">1</span> - threshy) / (lower_asymp))) ** (<span class="dv">1</span> / slope)
    m = (lower_asymp + upper_asymp - <span class="dv">1</span>) * np.exp(-(k * x / threshx) ** slope)
    <span class="kw">return</span> upper_asymp - m</code></pre>
<p>As you can see, this function has several more parameters to account for features of the data that may interest us.</p>
<p>Let’s see what that might look like:</p>
<pre class="sourceCode python"><code class="sourceCode python">
fig, ax = plt.subplots(<span class="dv">1</span>)
ax.plot(x, weibull(x, <span class="fl">0.5</span>, <span class="fl">3.5</span>, <span class="fl">0.5</span>, <span class="fl">0.95</span>), label=<span class="st">&#39;threshx=0.5, slope=3.5, lower_asymp=0.5, upper_asymp=0.95&#39;</span>)
ax.plot(x, weibull(x, <span class="fl">0.5</span>, <span class="fl">3.5</span>, <span class="dv">1</span>, <span class="fl">0.95</span>),  label=<span class="st">&#39;threshx=0.5, slope=3.5, lower_asymp=1, upper_asymp=0.95&#39;</span>)
ax.plot(x, weibull(x, <span class="fl">0.5</span>, <span class="fl">3.5</span>, <span class="dv">1</span>, <span class="fl">0.85</span>), label=<span class="st">&#39;threshx=0.5, slope=3.5, lower_asymp=1, upper_asymp=0.85&#39;</span>)
ax.plot(x, weibull(x, <span class="fl">0.25</span>, <span class="fl">3.5</span>, <span class="fl">0.5</span>, <span class="fl">0.95</span>), label=<span class="st">&#39;threshx=0.25, slope=3.5, lower_asymp=0.5, upper_asymp=0.95&#39;</span>)
ax.plot(x, weibull(x, <span class="fl">0.75</span>, <span class="fl">3.5</span>, <span class="dv">1</span>, <span class="fl">0.85</span>), label=<span class="st">&#39;threshx=0.75, slope=3.5, lower_asymp=1, upper_asymp=0.85&#39;</span>)
ax.grid(<span class="st">&#39;on&#39;</span>)
fig.set_size_inches([<span class="dv">14</span>, <span class="dv">8</span>])
plt.legend(loc=<span class="st">&#39;lower right&#39;</span>)</code></pre>
<div class="figure">
<img src="img/figure7.png" alt="Weibull functions" />
<p class="caption">Weibull functions</p>
</div>
<p>We go ahead and fit this function as well, using <code>curve_fit</code></p>
<pre class="sourceCode python"><code class="sourceCode python">
params_ortho, cov_ortho = opt.curve_fit(weibull, x_ortho, y_ortho)
params_para, cov_para = opt.curve_fit(weibull, x_para, y_para)</code></pre>
<p>And examine the results:</p>
<pre class="sourceCode python"><code class="sourceCode python">
y_ortho_hat = weibull(x, params_ortho[<span class="dv">0</span>], params_ortho[<span class="dv">1</span>], params_ortho[<span class="dv">2</span>], params_ortho[<span class="dv">3</span>])
fig, ax = plt.subplots(<span class="dv">1</span>)
ax.plot(x, y_ortho_hat)
ax.plot(x_ortho, y_ortho, <span class="st">&#39;bo&#39;</span>)

y_para_hat = weibull(x, params_para[<span class="dv">0</span>], params_para[<span class="dv">1</span>], params_para[<span class="dv">2</span>], params_para[<span class="dv">3</span>])
ax.plot(x, y_para_hat)
ax.plot(x_para, y_para, <span class="st">&#39;go&#39;</span>)

ax.grid(<span class="st">&quot;on&quot;</span>)
fig.set_size_inches([<span class="dv">8</span>, <span class="dv">8</span>])</code></pre>
<div class="figure">
<img src="img/figure8.png" alt="Weibull fits" />
<p class="caption">Weibull fits</p>
</div>
<p>Calculating the error, we find that the SSE is smaller for this model:</p>
<pre class="sourceCode python"><code class="sourceCode python">
SSE_para = np.<span class="dt">sum</span>((weibull(x_para, params_para[<span class="dv">0</span>], params_para[<span class="dv">1</span>], params_para[<span class="dv">2</span>], params_para[<span class="dv">3</span>]) - y_para) ** <span class="dv">2</span>)
SSE_ortho = np.<span class="dt">sum</span>((weibull(x_ortho, params_ortho[<span class="dv">0</span>], params_ortho[<span class="dv">1</span>], params_ortho[<span class="dv">2</span>], params_ortho[<span class="dv">3</span>]) - y_ortho) ** <span class="dv">2</span>)</code></pre>
<p>Should we switch over to this model? It seems to be doing better in fitting the data.</p>
<h2 id="overfitting">Overfitting</h2>
<p>One of the reasons we should be suspicious of the more accurate second model is tha the Weibull function has more parameters. This is sometimes referred to as the “degrees of freedom” of the model, and intuitively just means that the more parameters a function has, the more adjustments you can make to the estimated y values that you could consistently get across a range of x values. Let’s examine the most extreme case of that. Consider the case of fitting a polynomial of degree 6 to these data (7 parameters, including <span class="math">\(\beta_0\)</span>):</p>
<pre class="sourceCode python"><code class="sourceCode python">
beta_ortho = np.polyfit(x_ortho, y_ortho, <span class="dv">6</span>)
beta_para = np.polyfit(x_para, y_para, <span class="dv">6</span>)</code></pre>
<p>This model has even smaller error on the data:</p>
<pre class="sourceCode python"><code class="sourceCode python">
SE_ortho = np.<span class="dt">sum</span>((y_ortho - np.polyval(beta_ortho, x_ortho))**<span class="dv">2</span>)
SSE_para = np.<span class="dt">sum</span>((y_para - np.polyval(beta_para, x_para))**<span class="dv">2</span>)</code></pre>
<p>But examining the curves of the fits, you can see that it does some absurd things in order to fit the data:</p>
<div class="figure">
<img src="img/figure9.png" alt="Fits of polynomials with degree 6" />
<p class="caption">Fits of polynomials with degree 6</p>
</div>
<p>That is, it bends to match not only the large-scale trends in the data, but also the noise associated with each data point. This match to the noise that characterizes the sample is called “overfitting”.</p>
<h2 id="how-to-deal-with-overfitting-while-allowing-maximal-model-flexibility">How to deal with overfitting, while allowing maximal model flexibility</h2>
        </div>
      </div>
      </article>
      <div class="footer">
        <a class="label swc-blue-bg" href="https://github.com/arokem/scipy-optimize">Source</a>
        <a class="label swc-blue-bg" href="mailto:arokem@gmail.com">Contact</a>
        <a class="label swc-blue-bg" href="LICENSE.html">License</a>
      </div>
    </div>
    <!-- Javascript placed at the end of the document so the pages load faster -->
    <script src="http://software-carpentry.org/v5/js/jquery-1.9.1.min.js"></script>
    <script src="css/bootstrap/bootstrap-js/bootstrap.js"></script>
    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
  </body>
</html>
